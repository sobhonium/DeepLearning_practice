{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d76749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one is a total crab. read the next one.\n",
    "# Understanding autoencoders via a another example---> simple solid boxes in an image \n",
    "# This time I want to apply variational autoencoder (VAE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a73d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7df0b9a",
   "metadata": {},
   "source": [
    "## generate the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e07250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sobhan/miniconda3/envs/ml-dl/lib/python3.9/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n",
    "import torch.nn as nn\n",
    "import torch as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da315a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463b496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "832f0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def box(radius=2, box_size=28):\n",
    "    '''\n",
    "    to put some solid boxes in plain white images\n",
    "    returns image(s) with boxes on them with the radius defined for them\n",
    "    Comment: radius is a misnormer here. I wanted to show a scale thing for box in image\n",
    "    firstly I wrote a cricle function (not a box) that is why I left radius like this \n",
    "    '''\n",
    "    \n",
    "    if isinstance(radius, int):\n",
    "        rads = np.array([radius])\n",
    "    elif not isinstance(radius, np.ndarray):\n",
    "        rads = np.array(radius)\n",
    "        \n",
    "    rads = np.array(radius)\n",
    "\n",
    "    position = np.zeros((rads.size, box_size//2 * 2, box_size//2 * 2))\n",
    "    for i, r in enumerate(rads):\n",
    "        \n",
    "        for  x in range(-box_size//2, box_size//2):\n",
    "            for y in range(-box_size//2, box_size//2):\n",
    "                if ((abs(x) - r) < 0.01 and (abs(y) - r) < 0.01 ):\n",
    "                    position[i, x+box_size//2 , y+box_size//2] = 1\n",
    "   \n",
    "    return  torch.from_numpy(position).flatten(start_dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c07c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_siz = 10\n",
    "num_features = 150   # sample data number\n",
    "rand_radious= torch.randint(3, 50, (num_features,))/10 # create a random array of raduis\n",
    "features = box(radius=rand_radious, box_size=box_siz)  # data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47207d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65cb72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f39999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00b8dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = features # as an autoencoder we should have the same groud truth for output as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63414e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([150, 100]), torch.Size([150, 100]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4727d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0], labels[0] # see? they are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c5f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30693f40",
   "metadata": {},
   "source": [
    "## Reading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5bb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a4a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True): #@save\n",
    "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae2e8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81c1a5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f757a7eac10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e37aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is import to use iterator for that\n",
    "# next(iter(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee07d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0a65df7",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdff8c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = (box_siz//2 * 2)**2  # square of image box. Used when the entire image is flattened or reshaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "079f069e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7c8c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bottleneck here has ... nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5577b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.tensor([1,2,3]).float()\n",
    "# std = torch.exp(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00f301a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a70926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.randn_like(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a358fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ff596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ef6eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4303e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,512)\n",
    "        self.fc21 = nn.Linear(512, 10)\n",
    "        self.fc22 = nn.Linear(512, 10)\n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(10, 512)\n",
    "        self.fc4 = nn.Linear(512, input_size)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc21(x), self.fc22(x)\n",
    "        \n",
    "    def decode(self, z):\n",
    "        z = self.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(z))\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 *logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        outp = self.decode(z)\n",
    "#         print(outp)\n",
    "        return outp, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d757a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch as T\n",
    "\n",
    "# class Net(T.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.hid1 = T.nn.Linear(dim, 7)  \n",
    "#         self.oupt = T.nn.Linear(7, dim)\n",
    "    \n",
    "\n",
    "#         T.nn.init.xavier_uniform_(self.hid1.weight)\n",
    "#         T.nn.init.zeros_(self.hid1.bias)\n",
    "#         T.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "#         T.nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         z = T.tanh(self.hid1(x))\n",
    "#         z = self.oupt(z) \n",
    "        \n",
    "#         return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e07f44e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net()\n",
    "net = VariationalAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b48cf064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalAutoencoder(\n",
       "  (fc1): Linear(in_features=100, out_features=512, bias=True)\n",
       "  (fc21): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (fc22): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc3): Linear(in_features=10, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model:\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ecee0a",
   "metadata": {},
   "source": [
    "## Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f8ea420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net[0] has accesss to the first layer of net\n",
    "\n",
    "# net[0].weight.data.normal_(0, 0.01)\n",
    "# net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de58aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b74164e",
   "metadata": {},
   "source": [
    "## Defining the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44434b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c38e3280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_fn(x, recon_x, mu, logvar):\n",
    "    BCE = nn.functional. binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25b30c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "defee6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5109, 0.4580, 0.5003, 0.5627, 0.5680, 0.3785, 0.4814, 0.4127, 0.4702,\n",
       "         0.4908, 0.4988, 0.4210, 0.5497, 0.5168, 0.4361, 0.5375, 0.4732, 0.5803,\n",
       "         0.4796, 0.5303, 0.5207, 0.5648, 0.3986, 0.5807, 0.4446, 0.4149, 0.5404,\n",
       "         0.5108, 0.4786, 0.4675, 0.4106, 0.4222, 0.4923, 0.4836, 0.5075, 0.4712,\n",
       "         0.5036, 0.5171, 0.5652, 0.4504, 0.5013, 0.5096, 0.5027, 0.3844, 0.5814,\n",
       "         0.5071, 0.4867, 0.4074, 0.4895, 0.5539, 0.4840, 0.5267, 0.4726, 0.4477,\n",
       "         0.5318, 0.5791, 0.4887, 0.5226, 0.4454, 0.5814, 0.4350, 0.4545, 0.5661,\n",
       "         0.6739, 0.4993, 0.4099, 0.4406, 0.5756, 0.5576, 0.5081, 0.4725, 0.4940,\n",
       "         0.5450, 0.4761, 0.5053, 0.5411, 0.4537, 0.4501, 0.4711, 0.5592, 0.5179,\n",
       "         0.4829, 0.4630, 0.5081, 0.4948, 0.4772, 0.4173, 0.5295, 0.4725, 0.4556,\n",
       "         0.4984, 0.5530, 0.4739, 0.5341, 0.5382, 0.5118, 0.4494, 0.4586, 0.4909,\n",
       "         0.5760], grad_fn=<SigmoidBackward0>),\n",
       " tensor([ 0.0471, -0.0731, -0.0219, -0.0912,  0.0269, -0.0256,  0.0826,  0.1027,\n",
       "          0.0019, -0.0742], grad_fn=<AddBackward0>),\n",
       " tensor([-0.0503,  0.1052,  0.0680,  0.0016, -0.0192, -0.0332,  0.0068, -0.1145,\n",
       "          0.1031,  0.1436], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing loss function:\n",
    "X = features[0].float()\n",
    "y = X\n",
    "net(X)\n",
    "# loss(net(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a0bd5",
   "metadata": {},
   "source": [
    "## Defining the Optimization Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "727b6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization algo\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75d6ba01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f7559b83cf0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d1dccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69770a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a5a1fa",
   "metadata": {},
   "source": [
    "# evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef00af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8a9825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, dataloader, loss_fn=nn.MSELoss(), flatten=True, conditional=False):\n",
    "    losses = []\n",
    "    for batch, labels in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "#         if flatten:\n",
    "#             batch = batch.view(batch.size(0), 28*28)\n",
    "            \n",
    "#         if conditional:\n",
    "#             loss = loss_fn(batch, model(batch, labels))\n",
    "#         else:\n",
    "        loss = loss_fn(batch, model(batch.float()))\n",
    "            \n",
    "        losses.append(loss)\n",
    "\n",
    "    return (sum(losses)/len(losses)).item() # calculate mean\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(losses, autoencoder, dataloader, flatten=True, vae=False, conditional=False, title=\"\"):\n",
    "#     display.clear_output(wait=True)\n",
    "    if vae and conditional:\n",
    "        model = lambda x, y: autoencoder(x, y)[0]\n",
    "    elif vae:\n",
    "        model = lambda x: autoencoder(x)[0]\n",
    "    else:\n",
    "        model = autoencoder\n",
    "\n",
    "    loss = calculate_loss(model, dataloader, flatten=flatten, conditional=conditional)\n",
    "#     show_visual_progress(model, test_dataloader, flatten=flatten, vae=vae, conditional=conditional, title=title)\n",
    "    \n",
    "    losses.append(loss)\n",
    "#     print(losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2486f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee087177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "        X_test = features[20:40] #.flatten()\n",
    "        losses = []\n",
    "        loss_fn=nn.MSELoss()\n",
    "        for x in X_test:\n",
    "            y_hat, mu, var = model(x.float())\n",
    "#             loss_fn(model(x.float()), x.float())\n",
    "            l = loss_fn(y_hat, x.float())\n",
    "            losses.append(l)\n",
    "        print(sum(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a9d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "659a9135",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8e10fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataloader = load_array((features[10:20], labels[10:20]), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6225879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c29be450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c932f078",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         y_hat, mu, logvar \u001b[38;5;241m=\u001b[39m net(X\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         print(X.float(), y_hat)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m         l \u001b[38;5;241m=\u001b[39m \u001b[43mvae_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogvar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         trainer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m         l\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mvae_loss_fn\u001b[0;34m(x, recon_x, mu, logvar)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvae_loss_fn\u001b[39m(x, recon_x, mu, logvar):\n\u001b[0;32m----> 2\u001b[0m     BCE \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# see Appendix B from VAE paper:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# https://arxiv.org/abs/1312.6114\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     KLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m logvar \u001b[38;5;241m-\u001b[39m mu\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m logvar\u001b[38;5;241m.\u001b[39mexp())\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-dl/lib/python3.9/site-packages/torch/nn/functional.py:3083\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3080\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3081\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "validation_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for X,Y in data_iter:\n",
    "\n",
    "        #l = loss(net(X.float()), Y.float())\n",
    "        \n",
    "        # +\n",
    "        y_hat, mu, logvar = net(X.float())\n",
    "#         print(X.float(), y_hat)\n",
    "        l = vae_loss_fn(X.float(), y_hat, mu, logvar)\n",
    "\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step() \n",
    "    \n",
    "\n",
    "#     y_hat\n",
    "#     evaluate(validation_losses, net, test_dataloader, vae=True, title=f'VAE - Epoch {epoch}')\n",
    "    evaluate(net)\n",
    "    \n",
    "#         print(l.sum())\n",
    "        \n",
    "#         print(l.shape)\n",
    "        #optimizer\n",
    "#     l = (net(features.float()) - labels.float()).sum()\n",
    "# #     print(net[0].weight.data)\n",
    "#     if (epoch%10 == 0):\n",
    "#         print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34251978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = torch.tensor([1.15,1.1])\n",
    "# loss_fn(d,d+1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940cfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d8b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2258f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99547f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08290fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98421b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc8dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_siz//2 *2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49d4a4a",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape, mu.shape, var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a85921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how our model represent a data sample like this: [0.1, 0.2, 0.3, 0.4]\n",
    "sample_example = 2\n",
    "x = features[sample_example].reshape(box_siz//2 *2,box_siz//2 *2).float()\n",
    "y, mu, var = net(features[sample_example].float())\n",
    "# y = mu + y*var\n",
    "y = y.reshape(box_siz//2 *2,box_siz//2 *2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefe94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x # is also the ground truth as the same time (autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b924c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f65ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ededfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90750b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be863ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e13ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.imshow(x.detach().numpy(), cmap='Greys')\n",
    "plt.subplot(212)\n",
    "plt.imshow(y.detach().numpy(), cmap='Greys',  interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c25dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb22866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f72fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = features[50].reshape(box_siz//2 *2,box_siz//2 *2).float()\n",
    "y, mu, var = net(x.flatten())\n",
    "y = y.reshape(box_siz//2 *2,box_siz//2 *2).float()\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(x.detach().numpy(), cmap='Greys')\n",
    "plt.subplot(212)\n",
    "plt.imshow(y.detach().numpy(), cmap='Greys',  interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31b0a047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b170ed",
   "metadata": {},
   "source": [
    "## adding noise to the data to see how it can recognize the original one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6949048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0, 1., 0.],\n",
    "        [0., 0., 1., 1., 1., 1., 1., 0.],\n",
    "        [0., 0., 1., 0., 0.,0., 1., 0.],\n",
    "        [0., 0., 1.,0.,0.,0., 1., 0.],\n",
    "        [0., 1., 1.,0., 0, 0., 1., 0.],\n",
    "        [0., 0., 1., 1., 1., 1., 1., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
    "\n",
    "y, mu, var = net(x.flatten())\n",
    "y = y.reshape(box_siz//2 *2,box_siz//2 *2).float()\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(x.detach().numpy(), cmap='Greys')\n",
    "plt.subplot(212)\n",
    "plt.imshow(y.detach().numpy(), cmap='Greys',  interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(  [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                    [0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "                    [0., 0., 1., 1., 1., 1., 1., 0.],\n",
    "                    [0., 0., 1.,  1., 1., 1., 1., 0.],\n",
    "                    [0., 0., 1., 0., 0., 0., 1., 0.],\n",
    "                    [0., 0,  1., 0., 0., 0., 1., 0.],\n",
    "                    [0., 0., 1., 1., 1., 1., 1., 0.],\n",
    "                    [0., 0., 0.,  1., 1., 1., 0., 0.]])\n",
    "y, mu, var = net(x.flatten())\n",
    "y = y.reshape(box_siz//2 *2,box_siz//2 *2).float()\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.imshow(x.detach().numpy(), cmap='Greys')\n",
    "plt.subplot(212)\n",
    "plt.imshow(y.detach().numpy(), cmap='Greys',  interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8afe20",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31bd78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f18544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a rule of thumb, remember this: The smaller the bottleneck, the lower the risk of overfitting. \n",
    "# but not too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ee987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c44a345b",
   "metadata": {},
   "source": [
    "## Advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e659936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69887e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noww let's write a model with encoder and decoder like above but in an advance fashion understanding\n",
    "# I don't too much in this just split the mentioned model into encoder and decoder functions:\n",
    "# Also rename it from Net into Autoencoder class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Autoencoder(T.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.hid1 = T.nn.Linear(dim, 7)  \n",
    "        self.oupt = T.nn.Linear(7, dim)\n",
    "    \n",
    "\n",
    "        T.nn.init.xavier_uniform_(self.hid1.weight)\n",
    "        T.nn.init.zeros_(self.hid1.bias)\n",
    "        T.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "        T.nn.init.zeros_(self.oupt.bias)\n",
    "    \n",
    "    # +\n",
    "    def encoder(self, x):\n",
    "        return T.tanh(self.hid1(x))\n",
    "    \n",
    "    # +\n",
    "    def decoder(self, z):\n",
    "        return self.oupt(z)\n",
    "    \n",
    "    # *\n",
    "    def forward(self, x):\n",
    "        z = encoder(x)\n",
    "        z = decoder(z) \n",
    "        return z\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0aaed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE are good for finding the nonlinear relationships in data\n",
    "# They can be used when features are high and data samples are low (by reduction of the features \n",
    "# that is typically used in dimensinal reductions)---> In biology samples is used repetiviely\n",
    "# see: https://www.frontiersin.org/articles/10.3389/fgene.2019.01205/full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7b05d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

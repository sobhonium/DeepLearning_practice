{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a0cac2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules import Module\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690cc330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0dfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c785be6e",
   "metadata": {},
   "source": [
    "# EX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "27e0a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z):\n",
    "        y = torch.exp(z)\n",
    "        ctx.save_for_backward(y)    # let's save y as it's useful in backward()\n",
    "        return y\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # automatically, z' (derv of z) is computed and given in 'grad_output'\n",
    "        y, = ctx.saved_tensors\n",
    "        # so y' = z'*exp(z), where y=exp(z)--> grad_output * y \n",
    "        return grad_output * y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2097e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(4)\n",
    "output = Exp.apply(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "79c7363e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4388, 1.7422, 1.1160, 2.6243])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e75a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'save_for_backward' for saving \n",
    "# 'ctx.saved_tensors' for retrieving the save value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68feb16e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40266b52",
   "metadata": {},
   "source": [
    "# EX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "10b7de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z):\n",
    "        # Because we are saving one of the inputs use `save_for_backward`\n",
    "        # Save non-tensors and non-inputs/non-outputs directly on ctx Example: ctx.weight= inp_weight (not in \n",
    "        # this class example but at other places you can)\n",
    "        ctx.save_for_backward(z) # because we need z in df/dz = z'*2*z, otherwise, we can save y or anyting else\n",
    "        return z**2              # y = z**2\n",
    " \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        # A function support double backward automatically if autograd\n",
    "        # is able to record the computations performed in backward\n",
    "        z, = ctx.saved_tensors\n",
    "        return grad_out * 2 * z    # df/dz = z'*2*z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "05948307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use double precision because finite differencing method magnifies errors\n",
    "x = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\n",
    "torch.autograd.gradcheck(Square.apply, x)\n",
    "# Use gradcheck to verify second-order derivatives\n",
    "torch.autograd.gradgradcheck(Square.apply, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e4aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "120ad049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the two examples above. Once we needed to save z and once we needed to save y\n",
    "# to be used in backward() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a5a742df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other times you need to save more that 1 value:\n",
    "# if y=f(z)=sinh(z) then it might be useful to reuse exp(x) and exp(-x). In these cases you \n",
    "# do as followings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a96202",
   "metadata": {},
   "source": [
    "# EX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d8641a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sinh(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        expx = torch.exp(x)\n",
    "        expnegx = torch.exp(-x)\n",
    "        ctx.save_for_backward(expx, expnegx)\n",
    "        # In order to be able to save the intermediate results, a trick is to\n",
    "        # include them as our outputs, so that the backward graph is constructed\n",
    "        return (expx - expnegx) / 2, expx, expnegx\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out, _grad_out_exp, _grad_out_negexp):\n",
    "        expx, expnegx = ctx.saved_tensors\n",
    "        grad_input = grad_out * (expx + expnegx) / 2\n",
    "        # We cannot skip accumulating these even though we won't use the outputs\n",
    "        # directly. They will be used later in the second backward.\n",
    "        grad_input += _grad_out_exp * expx\n",
    "        grad_input -= _grad_out_negexp * expnegx\n",
    "        return grad_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bdc0e9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def sinh(x):\n",
    "    # Create a wrapper that only returns the first output\n",
    "    return Sinh.apply(x)[0]\n",
    "\n",
    "x = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\n",
    "torch.autograd.gradcheck(sinh, x)\n",
    "torch.autograd.gradgradcheck(sinh, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1517ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is saves two values in ctx as showed above.\n",
    "# In addition to this, look at the header of function:\n",
    "# def backward(ctx, grad_out, _grad_out_exp, _grad_out_negexp):\n",
    "# it simply says that there are 3 outputs given by forward() function\n",
    "# and their derivatives are these 3 argumetns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2070171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, you can all you want with these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4a2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8898e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e22dc25",
   "metadata": {},
   "source": [
    "# EX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "76f9b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing equation 2 in https://ieeexplore.ieee.org/document/8403889 \n",
    "# : y = Sign(z) but because its derivative at 0 is not defined we definetly need\n",
    "# to define customized backward(), otherwise using such activation function in neural\n",
    "# network is not applicable. We manually modify/define the derivative as: \n",
    "# dy/dz = {1,  -1<=z<=1\n",
    "#          0,  o.w}\n",
    "# This is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "80cf09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryActication(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z):\n",
    "        \n",
    "        ctx.save_for_backward(z) # saves z in y=f(z). z is an array for all output neurons.\n",
    "        y = torch.sign(z)        # y shows y in y=f(z). Here just I know the size is like z'size.              \n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        z, = ctx.saved_tensors\n",
    "        dy_dz = z.clone()\n",
    "        dy_dz[(dy_dz>=-1) * (dy_dz<=1)] = 1\n",
    "        dy_dz[(dy_dz!=1)] = 0\n",
    "        # print('dy/dz=', dy_dz)\n",
    "        return  dy_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2349d4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  1.,  1., -1., -1.], grad_fn=<BinaryActicationBackward>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,0.2,0.1,-0.2,-0.5])\n",
    "a.requires_grad=True\n",
    "output = BinaryActication.apply(a)\n",
    "output\n",
    "# it's impossible to check backward() from this simple test. It needs to be checked in a nn or with \n",
    "# checking mentioned above in previous class example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545df9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea8a1592",
   "metadata": {},
   "source": [
    "# EX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4d58d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Penalty(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    \n",
    "    # defines activation function\n",
    "    def forward(ctx, input, l1weight = 0.1):\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.l1weight = l1weight\n",
    "        return input\n",
    "    \n",
    "    #  defines the gradient formula of the activation funciton.\n",
    "    # It should return as many tensors as there were inputs\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_variables\n",
    "        grad_input = input.clone().sign().mul(ctx.l1weight)\n",
    "        grad_input+=grad_output\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e3e57345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  0.2000,  0.1000, -0.2000, -0.5000],\n",
       "       grad_fn=<L1PenaltyBackward>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = torch.tensor([1,0.2,0.1,-0.2,-0.5])\n",
    "a.requires_grad=True\n",
    "L1Penalty().apply(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563280f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9115a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in some examples above, and also in the  following class backward is unneccessay \n",
    "# as it's differentiable and autograd engine\n",
    "# takes care of it (it already knows what to do with that). But just for showing how it works\n",
    "# look at that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bbde73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinActivation(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z):\n",
    "        # y = sin(z)\n",
    "        y = torch.sin(z)\n",
    "        ctx.save_for_backward(y)\n",
    "        return y\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # automatically, z' (derv of z) is stored in grad_output\n",
    "        y, = ctx.saved_tensors\n",
    "        # so y' = z'*cos(z), where y=sin(z)\n",
    "        return grad_output * torch.cos(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58937a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fcf44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a471d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing equation 2 in https://ieeexplore.ieee.org/document/8403889 \n",
    "# :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "529c2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BinaryActication(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, z):\n",
    "        \n",
    "#         ctx.save_for_backward(z) # saves z in y=f(z). z is an array for all output neurons.\n",
    "#         y = z.clone()            # y shows y in y=f(z). Here just I know the size is like z'size.\n",
    "#         print('z=', z)\n",
    "#         for i, element in enumerate(z): \n",
    "#             if (element>=0):\n",
    "#                 y[i] = 1\n",
    "#             else:\n",
    "#                 y[i] = -1\n",
    "#         print('y=sign(z)', y)        \n",
    "#         return y\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         z, = ctx.saved_tensors\n",
    "#         dy_div_dz = z.clone()   # showing gradient by z\n",
    "#         for i, element in enumerate(z): \n",
    "#             if (element>=-1 and element<=1):\n",
    "#                 dy_div_dz[i] = 1\n",
    "#             else:\n",
    "#                 dy_div_dz[i] = 0\n",
    "#         print('dy/dz=', dy_div_dz)\n",
    "#         return  dy_div_dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "760cdd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.tensor([1,0.2,0.1,-0.2,-0.5])\n",
    "# a.requires_grad=True\n",
    "# output = BinaryActication.apply(a)\n",
    "# output\n",
    "# # output.retain_grad()\n",
    "# # output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "10d23f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.mul()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6a6bd",
   "metadata": {},
   "source": [
    "# Junks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4d67db69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1.5,3.2,-50.1,-0.2,-0.5])\n",
    "a[(a>=-1) * (a<=1)] = 1\n",
    "a[(a!=1)] = 0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b551b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a333034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6e2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88cbc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0d351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6823ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1aa71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6568281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d280f704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "df1cce7b",
   "metadata": {},
   "source": [
    "Read this (suggested):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3e25b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/extending-pytorch-with-custom-activation-functions-2d8b065ef2fa\n",
    "# https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981bbbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad909e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3867f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a931603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7663e8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "323b5ffa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "There are four possibilities depending on what you are looking for. You will need to ask yourself two questions:\n",
    "\n",
    "Q1) Will your activation function have learnable parameters?\n",
    "\n",
    "If yes, you have no choice but to create your activation function as an nn.Module class because you need to store those weights.\n",
    "\n",
    "If no, you are free to simply create a normal function, or a class, depending on what is convenient for you.\n",
    "\n",
    "Q2) Can your activation function be expressed as a combination of existing PyTorch functions?\n",
    "\n",
    "If yes, you can simply write it as a combination of existing PyTorch function and won't need to create a backward function which defines the gradient.\n",
    "\n",
    "If no you will need to write the gradient by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e433d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b165f53",
   "metadata": {},
   "source": [
    "# EX1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6a3a870",
   "metadata": {},
   "source": [
    "Example 1: SinActivation function\n",
    "\n",
    "The SiLU function f(x) = SinActivation(x) does not have any learned weights and can be written entirely with existing PyTorch functions, thus you can simply define it as a function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d9a267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinActivation(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SinActivation, self).__init__()\n",
    "        return\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78951716",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(4, 10, bias=False),\n",
    "        SinActivation(),\n",
    "        nn.Linear(10, 10, bias=True),\n",
    "        SinActivation(),\n",
    "        nn.Linear(10, 3, bias=False)        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f973c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2053, -0.1000,  0.2663], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.tensor([-0.0617, -0.0059,  0.0449, 0.104])\n",
    "model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ac873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "275fe639",
   "metadata": {},
   "source": [
    "# EX2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f3fb9c4",
   "metadata": {},
   "source": [
    "Example 2: SiLU with learned slope\n",
    "\n",
    "In this case you have one learned parameter, the slope, thus you need to make a class of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "03da57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedSiLU(nn.Module):\n",
    "    def __init__(self, slope = 1):\n",
    "        super().__init__()\n",
    "        self.slope = slope * torch.nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.slope * x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9d486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "07983f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4e558",
   "metadata": {},
   "source": [
    "## Other examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "48996e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following case the function is treated as if it's differntible, otherwise I\n",
    "# should implement the backward() function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c961080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom activation \n",
    "class Act(Module):\n",
    "    def forward(self, z):\n",
    "        temp = torch.zeros(z.shape)\n",
    "#         print(z)\n",
    "        for i, elm in enumerate(z):\n",
    "            if(elm > 0.0):\n",
    "                temp[i] = torch.tanh(elm)\n",
    "            else:\n",
    "                temp[i] = torch.tanh(elm)+1\n",
    "        \n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c86d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0b748177",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "        nn.Linear(10, 10, bias=False),\n",
    "        Act(),\n",
    "        nn.Linear(10, 10, bias=True),\n",
    "        Act(),\n",
    "        nn.Linear(10, 10, bias=False)        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d0d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "89bacf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3563, -0.0428,  0.3658,  0.1742,  0.8193,  0.2885, -0.5353,  0.2849,\n",
       "         0.1380, -0.5452], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.rand((10))\n",
    "model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a727e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4c71b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beware that I'm using Module class ( not Function as at the eariler section \n",
    "# in this notebook) in here that means I can use if in nn.Sequential like regualr \n",
    "# activation functions. But, when I implement an activation function and have a\n",
    "# subclass of torch.autograd.Function, I need to use '<class-name>.apply()' to use\n",
    "# it in nerual network layers. See bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c75c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba953b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27358f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5599d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's imagine that the custom function is not differentible: \n",
    "# if you worry about backpropagation you need to learn/use the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c92874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bb74edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Penalty(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    \n",
    "    # defines activation function\n",
    "    def forward(ctx, input, l1weight = 0.1):\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.l1weight = l1weight\n",
    "        return input\n",
    "    \n",
    "    #  defines the gradient formula of the activation funciton.\n",
    "#     It should return as many tensors as there were inputs\n",
    "    # It should return as many tensors as there were inputs\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_variables\n",
    "        grad_input = input.clone().sign().mul(ctx.l1weight)\n",
    "        grad_input+=grad_output\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0e35879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10,10)\n",
    "        self.fc2 = nn.Linear(10,6)\n",
    "        self.fc3 = nn.Linear(6,10)\n",
    "        self.fc4 = nn.Linear(10,10)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.penalty = L1Penalty()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.penalty.apply(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88ba01e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Model()\n",
    "a = torch.rand(50,10)\n",
    "b = model(a)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d76b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be50bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e29ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherit from Function\n",
    "class LinearFunction(torch.autograd.Function):\n",
    "\n",
    "    # Note that forward, setup_context, and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    def forward(input, weight, bias):\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    # inputs is a Tuple of all of the inputs passed to forward.\n",
    "    # output is the output of the forward().\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        input, weight, bias = inputs\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a39da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8122d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10,10)\n",
    "        self.fc2 = nn.Linear(10,6)\n",
    "        self.fc3 = nn.Linear(6,10)\n",
    "        self.fc4 = nn.Linear(10,10)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.custom_func = LinearFunction()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.custom_func.apply(x, 1)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24278c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cbb402ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearFunctionBackward' object has no attribute 'mm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Model()\n\u001b[1;32m      2\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m50\u001b[39m,\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(b\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning0/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[47], line 16\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m, in \u001b[0;36mLinearFunction.forward\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m----> 7\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m(weight\u001b[38;5;241m.\u001b[39mt())\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m         output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(output)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LinearFunctionBackward' object has no attribute 'mm'"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Model()\n",
    "a = torch.rand(50,10)\n",
    "b = model(a)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2aa7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6baa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430d9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "14e2b386",
   "metadata": {},
   "source": [
    "Don’t bother about backpropagation if you use autograd compatible operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21404237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357142fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a631169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b1a42a7",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98af9cd6",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/46509039/pytorch-define-custom-function\n",
    "https://stackoverflow.com/questions/61117361/how-to-use-custom-torch-autograd-function-in-nn-sequential-model\n",
    "https://stackoverflow.com/questions/55765234/pytorch-custom-activation-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71c486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
